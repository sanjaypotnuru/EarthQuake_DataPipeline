{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e38633b-dd70-49be-a6d5-543de0db14b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''from datetime import date, timedelta\n",
    "\n",
    "# Remove this before running Data Factory Pipeline\n",
    "start_date = date.today() - timedelta(1)\n",
    "\n",
    "silver_adls = \"abfss://silver@earthquakeadlstorage.dfs.core.windows.net/\"\n",
    "gold_adls = \"abfss://gold@earthquakeadlstorage.dfs.core.windows.net/\"\n",
    "\n",
    "silver_data = f\"{silver_adls}earthquake_events_silver/\"'''\n",
    "import json\n",
    "\n",
    "# Get base parameters\n",
    "dbutils.widgets.text(\"bronze_params\", \"\")\n",
    "dbutils.widgets.text(\"silver_params\", \"\")\n",
    "\n",
    "bronze_params = dbutils.widgets.get(\"bronze_params\")\n",
    "silver_params = dbutils.widgets.get(\"silver_params\")\n",
    "\n",
    "# Debug: Print the raw input values for troubleshooting\n",
    "print(f\"Raw bronze_params: {bronze_params}\")\n",
    "print(f\"Raw silver_params: {silver_params}\")\n",
    "\n",
    "# Parse the JSON string\n",
    "bronze_data = json.loads(bronze_params)\n",
    "\n",
    "# Access individual variables\n",
    "start_date = bronze_data.get(\"start_date\", \"\")\n",
    "end_date = bronze_data.get(\"end_date\", \"\")\n",
    "silver_adls = bronze_data.get(\"silver_adls\", \"\")\n",
    "gold_adls = bronze_data.get(\"gold_adls\", \"\")\n",
    "silver_data = silver_params\n",
    "\n",
    "# Debug: Print the extracted values for verification\n",
    "print(f\"Start Date: {start_date}, End Date: {end_date}\")\n",
    "print(f\"Silver ADLS Path: {silver_adls}, Gold ADLS Path: {gold_adls}\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86963bb8-b544-49d1-a6c0-0b8d469eb433",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import date, timedelta\n",
    "from pyspark.sql.functions import col, when, udf, unix_timestamp, lit\n",
    "from pyspark.sql.types import StringType\n",
    "import reverse_geocoder as rg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09f24424-9c5a-4b95-aeba-fdb111559701",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.parquet(silver_data).filter(col('time') > start_date)\n",
    "\n",
    "silver_path = f\"{silver_adls}earthquake_events_silver/\"\n",
    "\n",
    "# Read silver\n",
    "df = spark.read.parquet(silver_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8a0344d-5c46-4df3-b4ff-22cb258ae06b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert BIGINT time â†’ timestamp and filter\n",
    "df = df.withColumn(\"time_ts\", (col(\"time\")/1000).cast(\"timestamp\")) \\\n",
    "       .filter(col(\"time_ts\") > start_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf8cdaea-0761-4d88-8043-304393cadb5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.limit(100)   # testing only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "725c5050-92b6-42b5-ad72-33bd0bf816be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@udf(StringType())\n",
    "def get_country_code(lat, lon):\n",
    "    try:\n",
    "        result = rg.search((float(lat), float(lon)))[0].get('cc')\n",
    "        return result\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26a38988-cdb6-4078-8c53-bdc03023d35b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add country\n",
    "df = df.withColumn(\"country_code\", get_country_code(col(\"latitude\"), col(\"longitude\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b54c437-808e-4f72-b49b-8eea4f7cd3f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add significance class\n",
    "df = df.withColumn(\n",
    "        'sig_class',\n",
    "        when(col(\"sig\") < 100, \"Low\")\n",
    "        .when((col(\"sig\") >= 100) & (col(\"sig\") < 500), \"Moderate\")\n",
    "        .otherwise(\"High\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9a3910c-8d2c-4ba2-a4a8-3572d84bc376",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: string (nullable = true)\n |-- longitude: double (nullable = true)\n |-- latitude: double (nullable = true)\n |-- elevation: double (nullable = true)\n |-- title: string (nullable = true)\n |-- place_description: string (nullable = true)\n |-- sig: long (nullable = true)\n |-- mag: double (nullable = true)\n |-- magType: string (nullable = true)\n |-- time: long (nullable = true)\n |-- updated: long (nullable = true)\n |-- time_ts: timestamp (nullable = true)\n |-- country_code: string (nullable = true)\n |-- sig_class: string (nullable = false)\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f7b351d-760e-45eb-8dd5-66da8a04702a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_with_location_sig_class = \\\n",
    "                            df.\\\n",
    "                                withColumn('sig_class', \n",
    "                                            when(col(\"sig\") < 100, \"Low\").\\\n",
    "                                            when((col(\"sig\") >= 100) & (col(\"sig\") < 500), \"Moderate\").\\\n",
    "                                            otherwise(\"High\")\n",
    "                                            )\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e1bf5c0-ab4e-4857-b08d-e2b96b5691cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: string (nullable = true)\n |-- longitude: double (nullable = true)\n |-- latitude: double (nullable = true)\n |-- elevation: double (nullable = true)\n |-- title: string (nullable = true)\n |-- place_description: string (nullable = true)\n |-- sig: long (nullable = true)\n |-- mag: double (nullable = true)\n |-- magType: string (nullable = true)\n |-- time: long (nullable = true)\n |-- updated: long (nullable = true)\n |-- time_ts: timestamp (nullable = true)\n |-- country_code: string (nullable = true)\n |-- sig_class: string (nullable = false)\n\n"
     ]
    }
   ],
   "source": [
    "df_with_location_sig_class.printSchema()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e37eafff-23e6-4683-956b-051a0ee37a00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the transformed DataFrame to the Silver container\n",
    "gold_output_path = f\"{gold_adls}earthquake_events_gold/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9287ccef-d0a7-4c1a-81e2-5772b5dbe87b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Append DataFrame to Silver container in Parquet format\n",
    "df_with_location_sig_class.write.mode('append').parquet(gold_output_path)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Gold Notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}